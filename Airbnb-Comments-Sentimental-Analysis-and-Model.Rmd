---
title: "Airbnb Comments Analysis Project"
output: github_document
---

```{r setup, include = FALSE}
#Setup globally. To hide code, ignore error and store cache. 
knitr::opts_chunk$set(echo = FALSE, error = TRUE, messages = FALSE, cache = TRUE)
library(tidyverse)
library(here)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(discrim)
library(naivebayes)

set.seed(1234)
theme_set(theme_minimal())
```

**I pin-pointed two data sets for the analysis - one named 'listings', the other named 'reviews'. I will combine features of interests I need into one final dataset in order to do my text analysis.**

```{r data-import-and-cleaning, include = FALSE}
listing <- read_csv(here("data","listings.csv")) %>%
  # select price and id ONLY
  select(price, id) %>%
  # remove outliers, as I know from previously the upper range should be $340, median being $99
  filter(price <= 340) %>%
  # select only higher price and lower price resorts to restrict binary outcomes
  filter(price <= 20 | price >= 320) %>%
  # convert price into categorical factor 
  mutate(price = cut(price, breaks = c(0,100,340),
                        labels = c("Low","High"))) 
# str(review)

review <- read_csv(here("data","reviews.csv")) %>%
  select (listing_id, comments) %>%
  # remove empty comments
  drop_na(comments) 

df <- listing %>%
  # join the two datasets
  left_join(review, by = c("id" = "listing_id")) 

## final df contains only three variables of interests: price, id, comments
```
*The spectrum we use is from Bing.*
```{r sentimental-analysis-and-positive-plot}
sa <- df %>%
  # tokenize comments
  unnest_tokens(word , comments)  %>% 
  inner_join(get_sentiments("bing"))

# Price: low and high 
df_pos_neg <- sa %>%
  # generate frequency count for each category and sentiment
  group_by(price, sentiment) %>%
  count(word) %>%
  # extract 10 most frequent pos/neg words per cataegory
  group_by(price, sentiment) %>%
  slice_max(order_by = n, n = 10)

df_pos_neg %>%
  filter(sentiment == "positive") %>%
  mutate(word = reorder_within(word, n, price)) %>%
  ggplot(aes(word, n, col = price, fill = price)) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~ price, scales = "free_y") +
  labs(title = "Positive words used in comments",
       x = NULL,
       y = "Number of occurences") +
  coord_flip()

```
**Through above comparison, it is quite evident to see that positive words used to comments on high value Airbnb are usually with details explanation. For instance - 'spacious', 'beautiful', etc.. This is reasonbale that high value Airbnbs usually possess larger spacial functions and more luxurious decoration.**   
  
**Positive words most commenly used in low value Airbnb are relatively moderate. Isdead of extremely satisfactory adjectives like 'perfect', 'amazing', reviewers are more prone to 'well', 'good' and 'comfortable'.**

```{r sentimental-analysis-and-pnegative-plot}
df_pos_neg %>%
  filter(sentiment == "negative") %>%
  mutate(word = reorder_within(word, n, price)) %>%
  ggplot(aes(word, n, col = price, fill = price)) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~ price, scales = "free_y") +
  labs(title = "Negative words used in comments",
       x = NULL,
       y = "Number of occurences") +
  coord_flip()
```
**From the comparison above, we spotted that reviewers of Airbnb would in both scenarios report 'issues' 'problems' and something 'broken' through the platform. We observe from the exact count of these negative words that high value Airbnbs tend to have less negative comments.**  
  
**Now since I have initially visualized the data, I will try and see if I can make a model out of these comments for classify if the value of the Airbnb. I establish a workflow using a relatively straightforward type of classification model: naive Bayes.**

```{r prerequisites-of-model, include = TRUE, echo = TRUE}
split <- initial_split(df, strata = price, prop = .8)
train <- training(split)
test <- testing(split)

#recipe
rec <- recipe(price ~ comments, data = df) %>%
  step_tokenize(comments) %>%
  step_tokenfilter(comments, max_tokens = 500) %>%
  step_tfidf(comments)

# model
spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")

wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(spec)

# fit train data
wf %>%
  fit(train)

#evaluation with 10 folds cv
folds <- vfold_cv(data = train, strata = price)

(cv <- wf %>%
  fit_resamples(
    folds,
    control = control_resamples(save_pred = TRUE)
  ) %>% 
  collect_metrics(summarize = TRUE))
```

**We concluded from here that the model has the accuracy around 78%, which is pretty good. May this model help Airbnb improve their price control system then!!**